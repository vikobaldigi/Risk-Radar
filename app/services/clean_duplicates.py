# app/services/clean_duplicates.py

import os
from sqlalchemy import create_engine, text

# === CONFIGURATION ===
# Dynamically locate the SQLite database
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATABASE_URL = f"sqlite:///{BASE_DIR}/app/database/riskradar.db"

# Create the SQLAlchemy engine
engine = create_engine(DATABASE_URL, echo=False)


def clean_duplicates():
    """
    1. Remove duplicate rows (identical ticker & date), keeping the first entry by rowid.
    2. VACUUM the database to reclaim space.
    3. Create a unique index on (ticker, date) to prevent future duplicates.
    """
    # 1Ô∏è‚É£ Delete duplicates in a single transaction
    with engine.begin() as conn:
        result = conn.execute(text("""
            DELETE FROM stock_data
            WHERE rowid NOT IN (
                SELECT MIN(rowid)
                FROM stock_data
                GROUP BY ticker, date
            );
        """))
        print(f"üóëÔ∏è  Deleted {result.rowcount} duplicate rows.")

    # 2Ô∏è‚É£ VACUUM and 3Ô∏è‚É£ create unique index
    # VACUUM must run outside of an active transaction
    with engine.connect() as conn:
        conn.execute(text("VACUUM;"))
        print("üßπ  Database vacuumed.")
        conn.execute(text("""
            CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_ticker_date
            ON stock_data (ticker, date);
        """))
        print("üîß  Created unique index on (ticker, date).")


if __name__ == "__main__":
    print("üîç  Starting duplicate cleanup and optimization...")
    clean_duplicates()
    print("‚úÖ  Cleanup complete.")